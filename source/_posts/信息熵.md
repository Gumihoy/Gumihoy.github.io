---
title: 信息熵
date: 2018-11-10 08:01:01
categories: 
    - 信息论
tags:
    - 数学
    - 算法
    - 机器学习
    - 信息论
    - 信息熵
---


熵（英语：entropy）是接收的每条消息中包含的信息的平均量。

熵的概念最早起源于物理学，用于度量一个热力学系统的无序程度。在信息论里面，熵是对不确定性的测量

`结论：熵越大不确定性越大，熵最小是0`
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　———摘自《维基百科》

<!-- more -->


<br/>
<br/>

### 公式

$$
H(X)=-\sum_{i=0} P(X_{i}) \log_b P(X_{i})
$$

其中，$P(X_{i})$为$X=X_{i}$的概率
在这里b是对数所使用的底，通常是2、自然常数e或是10。当b = 2，熵的单位是bit；当b = e，熵的单位是nat；而当b = 10，熵的单位是Hart。

<br/>
### 范例

1、随机投掷一枚硬笔，假设两面不相同且出现正面、反面概率都一样，为$\frac{1}{2}$。信息熵为：

$$
H(X)= -\sum_{i=1}^2 \frac{1}{2} \log_2 \frac{1}{2} = - (\frac{1}{2} \log_2 \frac{1}{2} + \frac{1}{2} \log_2 \frac{1}{2}) = 1
$$

但是如果一枚硬币的两面完全相同，信息熵：0

2、


<br/>
### 应用场景

[ID3算法](../ID3算法) [C4.5算法](../C4.5算法)


<br/>

--- 
参考
[wikipedia-信息熵](https://en.wikipedia.org/wiki/Entropy_(information_theory)
[百度百科-信息熵](https://baike.baidu.com/item/%E4%BF%A1%E6%81%AF%E7%86%B5)